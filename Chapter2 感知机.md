# Chapter2 感知机

## 2.1感知机模型

输入空间(特征空间)：$X = R^n$

输出空间：$Y = \{+1,-1\}$

输入样本：$x\subseteq X$

输出样本：$y \in Y$
$$
f(x) = sign(w \cdot x + b )
$$

$$
sign(x) = \begin{cases}
+1 & x \geq 0\\
-1 & x < 0
\end{cases}
$$



感知机是一种线性分类模型，属于判别模型。

感知机有如下几何解释，线性方程
$$
w\cdot x + b = 0
$$
对应于特征空间$R^n$中的一个超平面S，其中w是超平面的法向量，b是超平面的截距。这个超平面将特征空间划分称为两个部分，位于两部分的点（特征向量）分别分为正负两类。

## 2.2 感知机学习策略

### 2.2.1 数据集的线性可分性

### 2.2.2 感知机学习策略

**假设数据集是线性可分的**

输入空间$R^n$中任意一点到超平面S的距离：
$$
\frac{1}{||w||}|w\cdot x_0 + b|
$$
误分类点到超平面的距离是
$$
-\frac{1}{||w||}y_i(w\cdot x_i+b)
$$


下面为感知机学习的经验风险函数，其中$M$为误分类点的集合
$$
L(w,b) = -\sum_{x_i\in M} y_i(w\cdot x_i + b)
$$


## 2.3 感知机学习算法

目标：求参数$w,b$，使得下面的损失函数极小化问题的解
$$
\min_{w,b}L(w,b) = -\sum_{x_i\in M}y_i(w\cdot x_i + b)
$$
采用梯度下降
$$
\Delta_wL(w,b) = -\sum_{x_i \in M}y_ix_i
$$

$$
\Delta_bL(w,b) = -\sum_{x_i\in M}y_i
$$



### 2.3.1 感知机学习算法的原始形式

输入：训练数据集$T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$，其中$x_i \in X = R^n,y\in Y=\{-1,+1\},i=1,2,...,N$，学习率$\eta (0<0\leq1)$；

输出：$w,b$，感知机模型$f(x) = sign(w\cdot x+b)$

（1） 选取初值$w_0,b_0$

（2）在训练数据集中选取数据$(x_i,y_i)$

（3）如果$y_i(w\cdot x_i+b\leq0)$
$$
w\leftarrow w+ \eta y_i x_i
$$

$$
b\leftarrow b + \eta y_i
$$
（4）转至2，直到训练集中没有误分类点



这种学习算法在直观上有如下的解释：当一个实例点被误分类，也就是说位于超平面的错误一侧的时候，调整$w,b$的值，使得分离超平面S向误分类点的一侧移动，以减少误分类点与超平面之间的距离，直到超平面越过误分类点使其被正确分类。

感知机学习算法由于采用不同的初值选取或者选取不同顺序的误分类点，解可以不同。

### 2.3.2 算法的收敛性

### 2.3.3 感知机学习算法的对偶形式

对偶形式的基本想法是，将$w,b$表示为实例$x_i$和标记$y_i$的线性组合的形式，通过求解其系数而求得$w,b$，$\alpha_i = n_i \eta$。

这里，$\alpha_i \geq 0 ,i =1,2,...,N$，当$\eta = 1$的时候，表示第i个实例点由于误分类而进行更新的次数。实例点更新的次数越多，意味着他距离分离超平面越近，也就越难正确的分类，换句话说，也就是这样的实例点对学习结果影响越大。


$$
w = \sum_{i=1}^N \alpha_i y_i x_i
$$

$$
b = \sum_{i=1}^N\alpha_i y_i
$$

输入：训练数据集$T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$，其中$x_i \in X = R^n,y\in Y=\{-1,+1\},i=1,2,...,N$，学习率$\eta (0<0\leq1)$；

输出：$\alpha,b$，感知机模型$f(x) = sign(\sum_{j=1}^N\alpha_jy_jx_j \cdot x+b)$，其中$\alpha = (\alpha_1,\alpha_2,...,\alpha_N)^T$

（1）$\alpha \leftarrow 0,b\leftarrow 0$

（2）在训练集中选取数据$x_i,y_i$

（3）如果$y_i(\sum_{j=1}^N\alpha_jy_jx_j\cdot x_i +b) \leq0$,
$$
\alpha_i \leftarrow \alpha_i+\eta
$$

$$
b \leftarrow b + \eta y_i
$$

（4）转至（2）直到没有误分类数据

## 2.4 other qustion

### 2.4.1 验证感知机为什么不能表示异或

​	将异或的四种组合方式写出来，然后利用反证的方法论证。

### 2.4.2 感知机对偶形式的意义

​	在对偶形式的感知机学习算法中，对于输入实例是否误判的条件变换为 $y_i(\sum_{j=1}^N\alpha_jy_jx_j\cdot x_i +b) \leq0$。注意到这里所有输入实例都仅仅以内积的形式出现，所以我们可以预先计算输入实例两两之间的内积，得到所谓的Gram矩阵 。这样一来每次做误判检测的时候我们直接在Gram矩阵里查表就能拿到内积 $x_ix_j$ ，所以这个误判检测的时间复杂度是$O(N)$ 。也就是说，对偶形式的感知机，把每轮迭代的时间复杂度的数据规模从特征空间维度n转移到了训练集大小N上，但增加了预先计算Gram矩阵的时间。所以对于维度高，数量少的训练数据，可以提高每次迭代的性能。





